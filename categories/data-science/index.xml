<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>data science on Cecilia Blog</title>
    <link>https://roofkitty.github.io/categories/data-science/</link>
    <description>Recent content in data science on Cecilia Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 26 Nov 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://roofkitty.github.io/categories/data-science/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>How does early stopping affect A/B experiment?</title>
      <link>https://roofkitty.github.io/2020/11/26/early-stopping/</link>
      <pubDate>Thu, 26 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://roofkitty.github.io/2020/11/26/early-stopping/</guid>
      <description>In the tech industry, we use controlled experiments to learn and make decisions at every step of product development, from design to algorithms. When the experiment is about a single change, such as changing the layout of the UI or using a different machine learning algorithm, the methodology is often called A/B testing. While is all fashionable and new to use A/B testing, the technique behind the scene, hypothesis testing, has been around for a long time.</description>
    </item>
    
  </channel>
</rss>
